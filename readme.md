

# ğŸ“˜ **Purchase Propensity Prediction (Customer Churn Model Using RFM & Machine Learning)**

Predicting whether a customer will return to purchase again based on past purchase behavior.

---

## ğŸ“Œ **Overview**

This project builds an **industry-level customer churn model** (also known as Purchase Propensity Prediction).
The goal is to predict whether a customer will **buy again soon**, using only their past transaction history.

We use **RFM features** (Recency, Frequency, Monetary) derived from raw transactional data and train **three ML models**:

* **Logistic Regression**
* **Random Forest**
* **XGBoost**

This project follows a *corrected pipeline* with **NO data leakage**, ensuring realistic and valid prediction performance.

---

## ğŸ›’ **Business Problem**

Companies like Amazon, Flipkart, Swiggy, and BigBasket need to identify:

* Which customers are likely to buy again soon?
* Which customers are becoming inactive or churning?
* Which customers deserve special offers or retention campaigns?

Raw transactional data does *not* directly provide churn information.
Therefore, we convert it into customer-level behavioral features using the **RFM framework**.

---

## ğŸ“‚ **Dataset**

**Source:** Online Retail II Dataset
[https://www.kaggle.com/datasets/mashlyn/online-retail-ii-uci](https://www.kaggle.com/datasets/mashlyn/online-retail-ii-uci)

This dataset contains:

* Invoice number
* Product description
* Quantity
* Price
* Timestamp
* Customer ID
* Country

The dataset does **not** contain a target variable.
We create the labels ourselves.

---

# ğŸ¯ **Target Variable: BuyAgain**

We define a simple business rule:

```
BuyAgain = 1  â†’ customer purchased within last 30 days  
BuyAgain = 0  â†’ customer inactive recently  
```

This approximates **customer churn**.

âœ” **Recency is used ONLY for target creation**
âŒ **Recency is NOT used as a feature (fixed leakage)**

This ensures the ML model cannot cheat.

---

# ğŸ§  **Feature Engineering (RFM)**

The following customer-level features are extracted:

### âœ” **Recency**

Days since last purchase.
Used **only to build the target**, NOT as an ML feature.

### âœ” **Frequency**

Number of unique invoices â†’ number of purchase occasions.

### âœ” **Monetary**

Total money spent by the customer:

```
Monetary = Î£ (Quantity Ã— UnitPrice)
```

These 3 features capture:

* Customer loyalty
* Spending power
* Likelihood of repeat purchase

---

# ğŸ§© **Final ML Features (Leakage-Free)**

Only:

```
Frequency, Monetary
```

are used as ML inputs.

Recency is EXCLUDED because it directly determines the target.

---

# ğŸ› ï¸ **Modeling Pipeline**

### Models Trained Separately:

* Logistic Regression
* Random Forest Classifier
* XGBoost Classifier

### Steps:

1. Preprocessing using StandardScaler
2. Train/test split with stratification
3. Model training
4. Evaluation using AUC, classification report, confusion matrix

---

# ğŸ“Š **Model Performance (Leakage Fixed)**

| Model                   | AUC Score |
| ----------------------- | --------- |
| **Logistic Regression** | **0.737** |
| Random Forest           | 0.672     |
| **XGBoost**             | **0.742** |

These scores are **realistic and valid** for churn prediction with limited features.

---

# ğŸ“œ **Key Learning: Fixing Data Leakage**

Originally, Recency was used both:

* To create the target
* As an input feature

This caused **artificially perfect AUC (1.00)**.
We FIXED it by removing Recency from training features.

This brings the model to realistic industry performance and demonstrates correct ML practices.

---

# ğŸ“ **Project Structure**

```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ online_retail_II.csv
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_RFM_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 02_churn_model_training.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ create_rfm.py
â”‚   â”œâ”€â”€ train_models.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

# â–¶ï¸ **How to Run**

```bash
pip install -r requirements.txt
python src/create_rfm.py
python src/train_models.py
```

or open the notebooks in Jupyter/Colab.

---

# ğŸš€ **Future Improvements**

Here are enhancements that can increase performance:

### âœ” Add Recency Groups (safe, no leakage)

Bin Recency into categories instead of using raw values.

### âœ” Use Interpurchase Gap Features

More powerful than simple RFM.

### âœ” Perform Hyperparameter Tuning

Especially for XGBoost & Random Forest.

### âœ” Add Customer-Level Features

* Number of unique products purchased
* Avg order value
* Days between purchases
* Country

### âœ” Deploy model as API (FastAPI/Flask)

---

# ğŸ **Conclusion**

This project demonstrates:

âœ” End-to-end ML pipeline using transaction data
âœ” Correct customer-level feature engineering (RFM)
âœ” Proper target creation for churn prediction
âœ” Avoidance of data leakage
âœ” Comparison of 3 ML models
âœ” Realistic business-grade evaluation



